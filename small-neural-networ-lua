local layers = 
{
  {name = 'input', values={0.5, -0.4, 0.3, 0.9}},
  {name = 'first-hidden', size = 3, values={}},
  {name = 'output', size = 2, values={}},
}
layers[1].size = #layers[1].values -- size = 4

-- update
for n_layer = 2, #layers do
  local layer = layers[n_layer]
  local prev_layer = layers[n_layer-1]
  layer.nods = layer.nods or {}
  for n_nod = 1, layer.size do
    layer.nods[n_nod] = layer.nods[n_nod] or {}
    local nod = layer.nods[n_nod]
    nod.bias = nod.bias or math.random(-100, 100)/100
    local summ = nod.bias
    for n_synaps = 1, prev_layer.size do
      nod[n_synaps] = nod[n_synaps] or math.random(-100, 100)/100
      local weight = nod[n_synaps]
      local input = prev_layer.values[n_synaps]
      summ = summ + weight*input
    end
    local activated = (summ > 0) and summ or 0 -- ReLU
    layer.values[n_nod] = activated
  end
end

print (unpack(layers[#layers].values)) -- result: 0.88288	0.40905


-- result:
--[[
layers[1].name = "input"
layers[1].values = {0.5, -0.4, 0.3, 0.9}
layers[1].size = 4
layers[2].name = "first-hidden"
layers[2].size = 3
layers[2].values = {0, 0, 1.6879999999999999}
layers[2].nods[1][1] = -0.47
layers[2].nods[1][2] = -0.25
layers[2].nods[1][3] = -0.11
layers[2].nods[1][4] = 0.5
layers[2].nods[1].bias = -0.74
layers[2].nods[2][1] = 0.4
layers[2].nods[2][2] = 0.16
layers[2].nods[2][3] = 0.21
layers[2].nods[2][4] = -0.98
layers[2].nods[2].bias = -0.94
layers[2].nods[3][1] = 0.62
layers[2].nods[3][2] = -0.78
layers[2].nods[3][3] = 0.5
layers[2].nods[3][4] = 0.04
layers[2].nods[3].bias = 0.88
layers[3].name = "output"
layers[3].size = 2
layers[3].values = {0, 1.3038400000000001}
layers[3].nods[1][1] = 0.25
layers[3].nods[1][2] = -0.78
layers[3].nods[1][3] = -0.97
layers[3].nods[1].bias = -0.74
layers[3].nods[2][1] = -0.71
layers[3].nods[2][2] = 0.03
layers[3].nods[2][3] = 0.18
layers[3].nods[2].bias = 1
]]
